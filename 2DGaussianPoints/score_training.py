'''
Author: Agnimitra Dasgupta
Date: November 11, 2024

Comments:
    This script trains a score-based model to learn a score function with 2 components.
    The training data was generated by Ricardo Baptista and is stored in the file "training_data.npy".

'''

## Importing necessary libraries
import torch, random, shutil, os, numpy as np, torch.nn as nn
import matplotlib.pyplot as plt
from scipy.spatial import Voronoi, voronoi_plot_2d
from scipy import integrate
import shutil
from ema_pytorch import EMA

# Setting up the plotting parameters
plt.rcParams.update({"text.usetex": True,"font.family": "sans-serif",'font.size': 18})

class VP:
    '''
       Variance preserving schedule
    '''
    def __init__(self, sigma_max, sigma_min):
        self.beta_min = sigma_min
        self.beta_max = sigma_max
    def _beta_t(self, t): 
        return self.beta_min + t*(self.beta_max - self.beta_min)
    def _alpha_t(self, t):
        return t*self.beta_min + 0.5 * t**2 * (self.beta_max - self.beta_min)
    def _drift(self, x, t):
        return -0.5*self._beta_t(t[:,None])*x
    def _marginal_prob_mean(self, t):
        return torch.exp(-0.5 * self._alpha_t(t))
    def _marginal_prob_std(self, t):
        return torch.sqrt(1 - torch.exp(-self._alpha_t(t)))
    def _diffusion_coeff(self, t):
        return torch.sqrt(self._beta_t(t))

class VE:
    '''
        Variance exploding schedule
    '''
    def __init__(self, sigma_max, sigma_min):
        self.sigma = sigma_max

    def _drift(self, x, t):
        return torch.zeros(x.shape)

    def _marginal_prob_mean(self, t):
        return torch.ones((1,))

    def _marginal_prob_std(self, t):
        return torch.sqrt((self.sigma**(2 * t) - 1.) / 2. / np.log(self.sigma))

    def _diffusion_coeff(self, t):
        return self.sigma**t
    
from score_models import GMM_score

class score_edm(torch.nn.Module):
    '''
        Neural network to learn the score function
        Results in the paper use the default settings of width=256, depth=2, activation=ReLU
        The time embedding we use has been adapted from here: https://jakiw.com/sgm_intro
    '''
    def __init__(self, device, width=256, depth=2, activation=nn.ReLU):
        super().__init__()
        
        self.width = width
        self.depth = depth
        self.activation = activation()
        
        net = []
        net.append(nn.Linear(2+4,self.width))
        net.append(self.activation)
        for _ in range(self.depth):
            net.append(nn.Linear(self.width,self.width))
            net.append(self.activation)
        net.append(nn.Linear(self.width,2))
        self.net = nn.Sequential(*net).to(device=device)
        
    def forward(self, x, t, mode=None):
        t = t.squeeze()
        embed = [t - 0.5, torch.cos(2*np.pi*t), torch.sin(2*np.pi*t), -torch.cos(4*np.pi*t)]
        embed = torch.stack(embed, dim=-1)         
        x_in = torch.cat([x, embed], dim=-1)
        score = self.net(x_in).to(torch.float32)
        return score
    
def loss_func(net, X, schedule):
    '''
        Denoising score matching loss function (un-regularized)
    '''
    t = torch.rand([X.shape[0], 1], device=X.device)
    noise = torch.randn_like(X) 
    mean = (schedule._marginal_prob_mean(t)).to(X.device)
    std  = (schedule._marginal_prob_std(t)).to(X.device)
    x_tilde = mean * X + std * noise
    score = net(x_tilde, t)
    loss = torch.mean((std*score + noise)**2, dim=(1))
    return loss.mean()

## Load and plot the training data
data = torch.from_numpy(np.load("training_data.npy")).float()
N = data.shape[0]
dim = data.shape[1]
vor = Voronoi(data)
plt.figure(figsize=(2,2))
fig = voronoi_plot_2d(vor)
plt.xlim(-3,3)
plt.ylim(-3,3)
plt.show()

# Set seed manually for reproducibility
random.seed(1)
np.random.seed(1)
torch.manual_seed(0)
torch.cuda.manual_seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# Training parameters
n_epochs = 30000    # number of epochs
lr = 1e-3           # learning rate
sigma_max = 5.0     # Variance parameter of the schedule
sigma_min = 0.001   # Minimum variance of the schedule
save_checkpoints_marks = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 2000, 4000, 6000, 8000, 10000, 30000]

device = 'cuda' if torch.cuda.is_available() else 'cpu'

width = 256 # Width of the score network 
depth = 2   # Depth of the score network

score_net = score_edm(device=device, width=width, depth=depth, activation=nn.ReLU)
score_ema = EMA(score_net, beta=0.9999)
optimizer = torch.optim.Adam(score_net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)
#schedule = VP(sigma_max=sigma_max, sigma_min=sigma_min)
schedule = VE(sigma_max=sigma_max, sigma_min=sigma_min)

expname = 'depth_'+str(depth)+'_epochs_'+str(n_epochs)+'_schedule_'+ schedule.__class__.__name__ + '_sigma_' + str(int(sigma_max))

# Create a directory to save the results
path_to_run = './exps/' + expname + '/'
if os.path.exists(path_to_run):
    shutil.rmtree(path_to_run)
os.makedirs(path_to_run)
os.makedirs(path_to_run+'checkpoints/')
os.makedirs(path_to_run+'figures/')

# Plot the schedule
t = torch.linspace(-100, 0, 1000).to(device)
t = torch.exp(t)
sigmas = schedule._marginal_prob_std(t).cpu().detach().numpy()
plt.figure(figsize=(4,2))
plt.plot(t.cpu().detach().numpy(), sigmas)
plt.xlim(0,1)
plt.ylim(0, sigma_max)
plt.xlabel(r'$t$')
plt.ylabel(r'$\sigma(t)$')
plt.show()

# Training loop
for epoch in range(n_epochs):
    score_net.train()
    X = data.to(device)  # increase batch size by repeating data
    idx = torch.randperm(X.shape[0]) # permut rows of X
    X = X[idx]
    loss = loss_func(score_net, X, schedule)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    score_ema.update()
        
    if (epoch+1) in save_checkpoints_marks:
        print("epoch: {}, loss: {}".format(epoch, loss.item()))
        states = [score_net.state_dict(),score_ema.state_dict(),optimizer.state_dict(),epoch]
        torch.save(states, path_to_run+'checkpoints/'+'checkpoint_'+str(epoch)+'.pth')
        torch.save(states, path_to_run+'checkpoints/'+'checkpoint.pth')
        
        
states = [score_net.state_dict(),score_ema.state_dict(), optimizer.state_dict(), n_epochs]
torch.save(states, path_to_run+'checkpoints/'+'checkpoint.pth')

# Probability flow ODE sampler for the reverse SDE
def odeint_sampler(score_net, schedule, latents, batch_size, device):
    init_T = 1.0 * torch.ones(batch_size)
    init_x = latents * schedule._marginal_prob_std(init_T)[:, None]
    
    def score_eval_wrapper(sample, time_steps, schedule):
        sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(latents.shape)
        with torch.no_grad():    
            score = score_net(sample, time_steps)

        return score.detach().cpu()
    
    def ode_func(t, x):        
        batch_time = torch.ones(batch_size) * t
        g = schedule._diffusion_coeff(batch_time)
        f = schedule._drift(x.reshape(latents.shape), batch_time)
        rhs = f - 0.5*(g**2)[:,None] * score_eval_wrapper(x, batch_time.to(device), schedule)
        return rhs.cpu().numpy().reshape((-1,)).astype(np.float64)
    
    # Run the black-box ODE solver
    err_tol = 1e-5
    t_eval = np.concatenate([10 ** (np.linspace(0, -5, 100)) , np.array([0.0])])
    res = integrate.solve_ivp(ode_func, (1.0, 0.0), init_x.reshape(-1).cpu().numpy(), rtol=err_tol, atol=err_tol, method='RK45', dense_output=True, t_eval=t_eval)  
    return (res.t, res.y)
    
    
batch_size = 100
latents = torch.randn(batch_size, dim)
samples_t, samples_x = odeint_sampler(score_net, schedule, latents, batch_size, device)

# Plot the trajectories
ctr = 1
for i in range(batch_size):
    lat_shape = [batch_size, 2, len(samples_t)]
    res_loc = torch.tensor(samples_x, device=latents.device, dtype=torch.float32).reshape(lat_shape)
    fig = voronoi_plot_2d(vor, show_vertices=False, line_width=2, point_size=13)
    plt.plot(res_loc[i,0,:].detach().numpy(), res_loc[i,1,:].detach().numpy(), '-r',linewidth=3)
    plt.xlabel('$x_1$')
    plt.ylabel('$x_2$')
    plt.xlim(-2.2,3)
    plt.ylim(-3,2.2)
    ctr += 2
    plt.tight_layout()
    plt.savefig(path_to_run+'figures/'+'ODE_trajectory_'+str(i)+'.pdf')
    plt.close('all')

# Plot the final samples
lat_shape = [batch_size, 2, len(samples_t)]
res_loc = torch.tensor(samples_x, device=latents.device, dtype=torch.float32).reshape(lat_shape)
final_samples = res_loc[:,:,-1].detach().numpy()
fig = voronoi_plot_2d(vor, show_vertices=False, line_width=2, point_size=13)
plt.scatter(final_samples[:,0], final_samples[:,1], marker='o', color='r', s=50, facecolors='none')
plt.xlabel('$x_1$')
plt.ylabel('$x_2$')
plt.xlim(-2.2,3)
plt.ylim(-3,2.2)
plt.tight_layout()
plt.savefig(path_to_run+'figures/'+'samples.pdf')
plt.close('all')
np.save(path_to_run+'figures/final_samples.npy', final_samples)

# Plot the histogram of the distance to the closest training data point
res_loc = torch.tensor(samples_x, device=latents.device, dtype=torch.float32).reshape(batch_size, 2, -1)

dist_to_closest = np.zeros((batch_size,))
for i in range(batch_size):
    dist_to_closest[i] = np.min(np.linalg.norm(data - final_samples[i,:], axis=1))

#plt.figure(figsize=(4,2))
plt.hist(dist_to_closest, bins=20, color='r', edgecolor='k')
plt.xlabel('Distance to closest training data point')
plt.ylabel('Frequency')
plt.xlim(0,1.0)
plt.ylim(0,30)
plt.tight_layout()
plt.savefig(path_to_run+'figures/'+'histogram.pdf')
plt.close('all')
np.save(path_to_run+'figures/dist_to_closest.npy', dist_to_closest)

# Plot the score function
idx = 0
x_pt_ = data[idx,:].reshape(1,2)
x_pt = x_pt_ + 0.1*torch.randn_like(x_pt_)
t_pts = 10**np.linspace(-5,0,100)
t_pts = torch.tensor(t_pts, device=device, dtype=torch.float32)
t_pts = torch.cat([torch.tensor([0.0], device=device, dtype=torch.float32),t_pts])
t_pts = t_pts[:,None]
x_pt2 = x_pt.repeat(t_pts.shape[0],1).to(device)
score_learn = score_ema(x_pt2, t_pts)
score_learn_norm = torch.zeros(t_pts.shape[0])
score_true_norm = torch.zeros(t_pts.shape[0])
gmm_train = GMM_score(torch.tensor(data, dtype=torch.float32), schedule._marginal_prob_mean, schedule._marginal_prob_std)
score_gmm_norm = torch.zeros(t_pts.shape[0])

for i in range(t_pts.shape[0]):
    score_learn_norm[i] = torch.norm(score_learn[i]).detach().cpu() 
    sigma = schedule._marginal_prob_std(t_pts[i]) + 1.0
    score_true_norm[i] = torch.norm( -1*(x_pt.to(device) - x_pt_.to(device))/sigma**2).detach().cpu()
    score_gmm = gmm_train(x_pt.detach().cpu(), t_pts[i].detach().cpu())
    score_gmm_norm[i] = torch.norm(score_gmm).detach().cpu()

plt.plot(t_pts.cpu().numpy(), score_learn_norm.cpu().numpy(), '-r', label='Learned score')
plt.plot(t_pts.cpu().numpy(), score_gmm_norm.cpu().numpy(), '-g', label='GMM score')
plt.ylabel(r'$|| \nabla_{x} \log p(x_t, t) ||_2$')
plt.yscale('log')
plt.xlim(0,1.0)
#plt.ylim(0,2.0)
plt.legend(loc='upper right')
plt.tight_layout()
plt.savefig(path_to_run+'figures/'+'score_comparison.pdf')


